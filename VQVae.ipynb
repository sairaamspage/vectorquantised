{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJrTagvdvokZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch import nn\n",
        "import torch.nn.functional\n",
        "import torchvision.datasets.MNIST\n",
        "import torchvision.transforms\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#vqa vae model\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VQVAE, self).__init__()\n",
        "        #encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "        nn.Conv2d(1, 16, kernel_size=4, stride=2, padding=1),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(16, 4, kernel_size=4, stride=2, padding=1),\n",
        "        nn.BatchNorm2d(4),\n",
        "        nn.ReLU()\n",
        "        )\n",
        "        self.commitment_factor = 0.2\n",
        "        self.before_quantisation = nn.Conv2d(4, 2, 1)\n",
        "        #codebook vectors\n",
        "        self.codebook = nn.Parameters(torch.randn(3, 2)*0.0001)\n",
        "        self.following_quantisation = nn.Conv2d(2, 4, 1)\n",
        "        #decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "        nn.ConvTranspose2d(4, 16, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ConvTranspose2d(16, 1, kernel_size=4, stride=2, padding=1),\n",
        "        nn.Tanh()\n",
        "        )\n",
        "    def forward(self, data):\n",
        "        #first encode\n",
        "        encode_data = self.encoder(data)\n",
        "        #set to dimensionality of codebook\n",
        "        q_input = self.before_quantisation(encode_data)\n",
        "        B, C, H, W = q_input.shape()\n",
        "        q_input = q_input.permute(0, 2, 3, 1)\n",
        "        q_input = q_input.view(B,H*W,C)\n",
        "        #find closest codebook\n",
        "        d = torch.cdist(q_input, self.codebook[None,:].repeat(q_input.size(0), 1, 1))\n",
        "        #find indices of the d\n",
        "        idx = torch.argmin(d, dim=-1)\n",
        "        #select the indices\n",
        "        q_out = torch.index_select(self.codebook, dim=0, idx.view(-1))\n",
        "        #reshape q_input to 2 dimensions\n",
        "        q_input = q_input.continguous().view(-1, q_input.size(-1))\n",
        "        #get commitment loss -- move the input to the codebook vectors\n",
        "        commit_loss = torch.mean((q_out.detach() - q_input)**2)\n",
        "        #get code loss -- move codebook to the the input\n",
        "        code_loss = torch.mean((q_out - q_input.detach())**2)\n",
        "        #loss\n",
        "        q_loss = code_loss + self.commit*commit_loss\n",
        "        #to ensure gradient flow, apply the following use of detach()\n",
        "        q_out = q_input + (q_out - q_input).detach()\n",
        "        #reshape to B, C, H, W\n",
        "        q_out = q_out.view(B, H, W, C).permute(0, 3, 1, 2).contiguous()\n",
        "        #reshape idx also\n",
        "        #idx = idx.view(-1, q_out.size(-2), q_out.size(-1))\n",
        "        #generate image\n",
        "        decoded = self.following_quantisation(q_out)\n",
        "        decoded = self.decoder(decoded)\n",
        "        return decoded, q_loss, idx"
      ],
      "metadata": {
        "id": "twd3N_gqwIr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lstm to sample from vqvae\n",
        "class LSTMSampler(nn.Module):\n",
        "    def __init__(self, codebook=2, hidden_size=4, num_codebook=3):\n",
        "        self.codebook = codebook\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_codebook = num_codebook\n",
        "        self.embedding = nn.Embedding(num_codebook, codebook)\n",
        "        self.lstm = nn.LSTM(input_size=codebook, hidden_size=4, batch_size=True)\n",
        "        self.linear = nn.Linear(hidden_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear1 = nn.Linear(hidden_size, codebook)\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.lstm(x)\n",
        "        x = x[:, -1, :]\n",
        "        x = self.linear(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear1(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "0ZxknsMTIZH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train code\n",
        "def train(model, epochs, criterion, optimiser, scheduler, train_loader):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        print('Training epoch ', epoch)\n",
        "        for d, label in range(train_loader):\n",
        "            d = d.to(device)\n",
        "            optimiser.zero_grad()\n",
        "            decoded, q_loss, _ = model(d)\n",
        "            loss = criterion(decoded, d) + q_loss\n",
        "            loss.backward()\n",
        "            optimiser.step()"
      ],
      "metadata": {
        "id": "wrIg-6A0D3Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sample from trained model\n",
        "def train_sampler(gen_model, model, epochs, criterion, optimiser, train_loader):\n",
        "    gen_model.eval()\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        print('Training epoch', epoch)\n",
        "        for d, label in range(train_loader):\n",
        "            d = d.to(device)\n",
        "            with torch.no_grad():\n",
        "                _, _, en = gen_model(d)\n",
        "            x = en[:, :-1]\n",
        "            y = en[:, 1:]\n",
        "            optimiser.zero_grad()\n",
        "            o = model(x)\n",
        "            loss = criterion(y, o)\n",
        "            loss.backward()\n",
        "            optimiser.step()"
      ],
      "metadata": {
        "id": "MXtegNJkTHXr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}